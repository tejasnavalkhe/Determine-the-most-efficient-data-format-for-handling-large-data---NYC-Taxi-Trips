{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7601bf0e-75f1-4b25-97f1-54e1d855bd12",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# [CSC8101] Engineering for AI - 2024 Spark Coursework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a451fe1b-c36e-4226-a708-a903f5844f7d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Coursework overview\n",
    "\n",
    "### Inputs\n",
    "\n",
    "- **NYC Taxi Trips dataset** - list of recorded taxi trips, each with several characteristics, namely: distance, number of passengers, origin zone, destination zone and trip cost (total amount charged to customer).\n",
    "- **NYC Zones dataset** - list of zones wherein trips can originate/terminate.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. Data cleaning\n",
    "  1. Remove \"0 distance\" and 'no passengers' records.\n",
    "  2. Remove outlier records. \n",
    "2. Add new columns\n",
    "  1. Join with zones dataset\n",
    "  2. Compute the unit profitability of each trip\n",
    "3. Zone summarisation and ranking\n",
    "  1. Summarise trip data per zone\n",
    "  2. Obtain the top 10 ranks according to:\n",
    "    1. The total trip volume\n",
    "    2. Their average profitabilitiy\n",
    "    3. The total passenger volume\n",
    "4. Record the total and task-specific execution times for each dataset size and format.\n",
    "\n",
    "### How to\n",
    "\n",
    "###### Code structure and implementation\n",
    "\n",
    "- You must implement your solution to each task in the provided function code skeleton.\n",
    "- The task-specific functions are combined together to form the full pipeline code, executed last (do not modify this code).\n",
    "- Before implementing the specified function skeleton, you should develop and test your solution on separate code cells (create and destroy cells as needed).\n",
    "\n",
    "###### Development\n",
    "\n",
    "- Develop an initial working solution for the 'S' dataset and only then optimise it for larger dataset sizes.\n",
    "- To perform vectorised operations on a DataFrame:\n",
    "  - use the API docs to look for existing vectorised functions in: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html\n",
    "  - actions to get around the lazy execution of spark: \n",
    "  https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions\n",
    "  - if a customised function is required (e.g. to add a new column based on a linear combination of other columns), implement your own User Defined Function (UDF). See:  https://spark.apache.org/docs/latest/sql-ref-functions-udf-scalar.html\n",
    "- Use only the `pyspark.sql` API - documentation link below - (note that searching through the docs returns results from the `pyspark.sql` API together with the `pyspark.pandas` API):\n",
    "  - https://spark.apache.org/docs/3.2.0/api/python/reference/pyspark.sql.html\n",
    "- Periodically download your notebook to your computer as backup and safety measure against accidental file deletion.\n",
    " \n",
    "###### Execution time measurement\n",
    "\n",
    "- Execution time is calculated and returned by the Spark Engine and shown in the output region of the cell.\n",
    "- To measure the execution time of a task you must perform a `collect` or similar operation (e.g. `take`) on the returned DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d26c1c73-cac4-4937-8201-4c15cc6be296",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 0 - Read data\n",
    "\n",
    "The code below is ready to run. **Do not modify this code**. It does the following:\n",
    "\n",
    "- Reads the 'zones' dataset into variable 'zone_names'\n",
    "- Defines the `init_trips` function that allows you to read the 'trips' dataset (from the DBFS FileStore) given the dataset size ('S' to 'XXL') and format ('parquet' or 'delta') as function arguments\n",
    "- Defines the `pipeline` function, called in Task 4 to measure the execution time of the entire data processing pipeline\n",
    "- Shows you how to call the `init_trips` function and display dataset characteristics (number of rows, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57aaebcc-fc2a-4d86-a58e-c1489efeadaf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## global imports\n",
    "import pyspark.sql as ps\n",
    "import pyspark.sql.functions as pf\n",
    "import pandas as pd\n",
    "\n",
    "# Load zone names dataset - (much faster to read small file from git than dbfs)\n",
    "zones_file_url = 'https://raw.githubusercontent.com/mutazb999/CSC8101-lab-and-coursework/main/02-assignment-spark/taxi_zone_names.csv'\n",
    "zone_names = spark.createDataFrame(pd.read_csv(zones_file_url))\n",
    "\n",
    "# Function to load trips dataset by selected dataset size\n",
    "def init_trips(size = 'S', data_format = \"parquet\", taxi_folder = \"/FileStore/tables/taxi\"):     \n",
    "    \n",
    "    files = {\n",
    "        'S'  : ['2021_07'],\n",
    "        'M'  : ['2021'],\n",
    "        'L'  : ['2020_21'],\n",
    "        'XL' : ['1_6_2019', '7_12_2019'],\n",
    "        'XXL': ['1_6_2019', '7_12_2019', '2020_21']\n",
    "    }\n",
    "    \n",
    "    # validate input dataset size\n",
    "    if size not in files.keys():\n",
    "        print(\"Invalid input dataset size. Must be one of {}\".format(list(files.keys())))\n",
    "        return None               \n",
    "    \n",
    "    if data_format == \"parquet\":\n",
    "        filenames = list(map(lambda s: f'{taxi_folder}/parquet/tripdata_{s}.parquet', files[size]))\n",
    "        trips_df = spark.read.parquet(filenames[0])\n",
    "        \n",
    "        for name in filenames[1:]:\n",
    "            trips_df = trips_df.union(spark.read.parquet(name))\n",
    "            \n",
    "    elif data_format == \"delta\":\n",
    "        filenames = f\"{taxi_folder}/delta/taxi-{size}-delta/\"\n",
    "        trips_df = spark.read.format(\"delta\").load(filenames)\n",
    "    \n",
    "    else:\n",
    "        print(\"Invalid data format. Must be one of {}\".format(['parquet', 'delta']))\n",
    "        return None\n",
    "        \n",
    "    print(\n",
    "    \"\"\"\n",
    "    Trips dataset loaded!\n",
    "    ---\n",
    "      Size: {s}\n",
    "      Format: {f}\n",
    "      Tables loaded: {ds}\n",
    "      Number of trips (dataset rows): {tc:,}\n",
    "    \"\"\".format(s = size, f = data_format, ds = filenames, tc = trips_df.count()))\n",
    "    \n",
    "    return trips_df\n",
    "\n",
    "# helper function to print dataset row count\n",
    "def print_count(df):\n",
    "    print(\"Row count: {t:,}\".format(t = df.count()))\n",
    "\n",
    "def pipeline(trips_df, with_task_12 = False, zones_df = zone_names):\n",
    "    # Do not edit\n",
    "    #---\n",
    "\n",
    "    ## Task 1.1\n",
    "    _trips_11 = t11_remove_zeros(trips_df)\n",
    "\n",
    "    ## Task 1.2\n",
    "    if with_task_12:\n",
    "        _trips_12 = t12_remove_outliers(_trips_11)\n",
    "    else:\n",
    "        _trips_12 = _trips_11\n",
    "\n",
    "    ## Task 2.1\n",
    "    _trips_21 = t21_join_zones(_trips_12, zones_df = zone_names)\n",
    "\n",
    "    ## Task 2.2\n",
    "    _trips_22 = t22_calc_profit(_trips_21)\n",
    "\n",
    "    ## Task 3.1\n",
    "    _graph = t31_summarise_trips(_trips_22)\n",
    "\n",
    "    ## Task 3.2\n",
    "    _zones = t32_summarise_zones_pairs(_graph)\n",
    "\n",
    "    _top10_trips     = t32_top10_trips(_zones)\n",
    "    _top10_profit    = t32_top10_profit(_zones)\n",
    "    _top10_passenger = t32_top10_passenger(_zones)\n",
    "    \n",
    "    return([_top10_trips, _top10_profit, _top10_passenger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "490ab841-fd73-4466-a6e7-5121395deca9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n    Trips dataset loaded!\n    ---\n      Size: S\n      Format: parquet\n      Tables loaded: ['/FileStore/tables/taxi/parquet/tripdata_2021_07.parquet']\n      Number of trips (dataset rows): 2,898,033\n    \n"
     ]
    }
   ],
   "source": [
    "# CHANGE the value of argument 'size' to record the pipeline execution times for increasing dataset sizes\n",
    "SIZE = 'S'\n",
    "DATA_FORMAT = 'parquet'\n",
    "\n",
    "# Load trips dataset\n",
    "trips = init_trips(SIZE, DATA_FORMAT)\n",
    "\n",
    "# uncomment line only for small datasets\n",
    "# trips.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92da0d12-33a0-4df0-b1d2-607ebb65d948",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 2,898,033\n"
     ]
    }
   ],
   "source": [
    "print_count(trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "490e67f7-4457-4bd5-b89a-a6d24c5645d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- index: long (nullable = true)\n |-- VendorID: double (nullable = true)\n |-- tpep_pickup_datetime: string (nullable = true)\n |-- tpep_dropoff_datetime: string (nullable = true)\n |-- passenger_count: double (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- RatecodeID: double (nullable = true)\n |-- store_and_fwd_flag: string (nullable = true)\n |-- PULocationID: long (nullable = true)\n |-- DOLocationID: long (nullable = true)\n |-- payment_type: double (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- extra: double (nullable = true)\n |-- mta_tax: double (nullable = true)\n |-- tip_amount: double (nullable = true)\n |-- tolls_amount: double (nullable = true)\n |-- improvement_surcharge: double (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- congestion_surcharge: double (nullable = true)\n |-- cab_type: string (nullable = true)\n |-- lpep_pickup_datetime: string (nullable = true)\n |-- lpep_dropoff_datetime: string (nullable = true)\n |-- ehail_fee: double (nullable = true)\n |-- trip_type: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# dataset schemas\n",
    "trips.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e2ab160-b9b8-4c4a-bb70-fc3197bae230",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>PULocationID</th><th>DOLocationID</th><th>trip_distance</th><th>passenger_count</th><th>total_amount</th></tr></thead><tbody><tr><td>90</td><td>68</td><td>0.8</td><td>1.0</td><td>8.8</td></tr><tr><td>113</td><td>90</td><td>0.9</td><td>1.0</td><td>8.8</td></tr><tr><td>88</td><td>232</td><td>2.8</td><td>1.0</td><td>13.8</td></tr><tr><td>79</td><td>249</td><td>1.4</td><td>1.0</td><td>12.3</td></tr><tr><td>142</td><td>238</td><td>2.0</td><td>0.0</td><td>12.3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         90,
         68,
         0.8,
         1.0,
         8.8
        ],
        [
         113,
         90,
         0.9,
         1.0,
         8.8
        ],
        [
         88,
         232,
         2.8,
         1.0,
         13.8
        ],
        [
         79,
         249,
         1.4,
         1.0,
         12.3
        ],
        [
         142,
         238,
         2.0,
         0.0,
         12.3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "PULocationID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DOLocationID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "trip_distance",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "passenger_count",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "total_amount",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(trips[['PULocationID', 'DOLocationID', 'trip_distance', 'passenger_count', 'total_amount']].take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eee4451c-b49a-42a6-b87e-81aad4144cf1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- LocationID: long (nullable = true)\n |-- Borough: string (nullable = true)\n |-- Zone: string (nullable = true)\n |-- service_zone: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "zone_names.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a1ed0a0-0621-471d-8332-d47b52f71817",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>LocationID</th><th>Borough</th><th>Zone</th><th>service_zone</th></tr></thead><tbody><tr><td>1</td><td>EWR</td><td>Newark Airport</td><td>EWR</td></tr><tr><td>2</td><td>Queens</td><td>Jamaica Bay</td><td>Boro Zone</td></tr><tr><td>3</td><td>Bronx</td><td>Allerton/Pelham Gardens</td><td>Boro Zone</td></tr><tr><td>4</td><td>Manhattan</td><td>Alphabet City</td><td>Yellow Zone</td></tr><tr><td>5</td><td>Staten Island</td><td>Arden Heights</td><td>Boro Zone</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "EWR",
         "Newark Airport",
         "EWR"
        ],
        [
         2,
         "Queens",
         "Jamaica Bay",
         "Boro Zone"
        ],
        [
         3,
         "Bronx",
         "Allerton/Pelham Gardens",
         "Boro Zone"
        ],
        [
         4,
         "Manhattan",
         "Alphabet City",
         "Yellow Zone"
        ],
        [
         5,
         "Staten Island",
         "Arden Heights",
         "Boro Zone"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "LocationID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Borough",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Zone",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "service_zone",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(zone_names.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99294dd5-92b7-4e50-9623-d00d82da993b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 1 - Filter rows\n",
    "\n",
    "**Input:** trips dataset\n",
    "\n",
    "### Task 1.1 - Remove \"0 distance\" and 'no passengers' records\n",
    "\n",
    "Remove dataset rows that represent invalid trips:\n",
    "\n",
    "- Trips where `trip_distance == 0` (no distance travelled)\n",
    "- Trips where `passenger_count == 0` and `total_amount == 0` (we want to retain records where `total_amount` > 0 - these may be significant as the taxi may have carried some parcel, for example)\n",
    "\n",
    "Altogether, a record is removed if it satisfies the following conditions:\n",
    "\n",
    "`trip_distance == 0` or `(passenger_count == 0` and `total_amount == 0)`.\n",
    "\n",
    "**Recommended:** Select only the relevant dataset columns for this and subsequent tasks: `['PULocationID', 'DOLocationID', 'trip_distance', 'passenger_count', 'total_amount')]`\n",
    "\n",
    "### Task 1.2 - Remove outliers using the modified z-score\n",
    "\n",
    "Despite having removed spurious \"zero passengers\" trips in task 1.1, columns `total_amount` and `trip_distance` contain additional outlier values that must be identified and removed.\n",
    "\n",
    "To identify and remove outliers, you will use the modified [z-score](https://en.wikipedia.org/wiki/Standard_score) method.\n",
    "The modified z-score uses the median and [Median Absolute Deviation](https://en.wikipedia.org/wiki/Median_absolute_deviation) (MAD), instead of the mean and standard deviation, to determine how far an observation (indexed by i) is from the mean:\n",
    "\n",
    "$$z_i = \\frac{x_i - \\mathit{median}(\\mathbf{x})}{\\mathbf{MAD}},$$\n",
    "\n",
    "where x represents the input vector, xi is an element of x and zi is its corresponding z-score. In turn, the MAD formula is:\n",
    "\n",
    "$$\\mathbf{MAD} = 1.483 * \\mathit{median}(\\big\\lvert x_i - \\mathit{median}(\\mathbf{x})\\big\\rvert).$$\n",
    "\n",
    "Observations with **high** (absolute) z-score are considered outlier observations. A score is considered **high** if its __absolute z-score__ is larger than a threshold T = 3.5:\n",
    "\n",
    "$$\\big\\lvert z_i \\big\\rvert > 3.5.$$\n",
    "\n",
    "where T represents the number of unit standard deviations beyond which a score is considered an outlier ([wiki](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule)).\n",
    "\n",
    "This process is repeated twice, once for each of the columns `total_amount` and `trip_distance` (in any order).\n",
    "\n",
    "**Important:** Use the surrogate function [`percentile_approx`](https://spark.apache.org/docs/3.2.0/api/python/reference/api/pyspark.sql.functions.percentile_approx.html?highlight=percentile#pyspark.sql.functions.percentile_approx) to estimate the median (calculating the median values for a column is expensive as it cannot be parallelised efficiently)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b53021ef-a0ab-43be-8885-142617707a5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# develop your solution here (create/destroy cells as needed) and then implement it in the functions below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a68f7b9-1a88-485a-ab66-1a435fffa920",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### Task 1.1\n",
    "\n",
    "The function **t11_remove_zeros**, provided below, operates on a dataframe. Initially, it extracts specified column names such as *'PULocationID', 'DOLocationID', 'trip_distance', 'passenger_count', 'total_amount'*. Subsequently, it eliminates rows where *trip_distance* equals zero (0) or where *passenger_count* equals zero only when *total_amount* exceeds zero (indicating cargo transportation with a charged amount despite zero passengers). Furthermore, the function filters out rows where both *passenger_count* and *total_amount* are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ac2d5d7-21d4-4bbb-b2ea-0e6e4df89a58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Your solution implementation to task 1.1 goes HERE\n",
    "def t11_remove_zeros(df):\n",
    "    # input: trips dataset\n",
    "    df = trips.select('PULocationID', 'DOLocationID', 'trip_distance', 'passenger_count', 'total_amount')\n",
    "    df = df.filter(\n",
    "        (df.trip_distance != 0) & \n",
    "        (\n",
    "            ((df.total_amount > 0) & (df.passenger_count == 0)) | ((df.passenger_count != 0) & (df.total_amount != 0))\n",
    "        )\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c6c287d-8707-4989-8ee9-2967a486c925",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 2,693,786\n"
     ]
    }
   ],
   "source": [
    "# execute task 1.1\n",
    "trips_11 = t11_remove_zeros(trips)\n",
    "\n",
    "print_count(trips_11)\n",
    "\n",
    "## uncomment only for smaller datasets\n",
    "# display(trips_11.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "756f7a49-cc58-4329-a0f7-2e9ed664df32",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### Task 1.2\n",
    "\n",
    "The following function operates on a dataframe to *filter out outliers*. Initially, it computes the *median values for trip distance and total amount*. Then two additional columns are created: *\"total_amount_median_diff\" and \"trip_distance_median_diff\",* capturing the difference between the actual values and their respective medians. Moving forward, **Median Absolute Deviation (MAD)** is calculated for these differences (by using *percentile_approx*), denoted as *total_amount_MAD* and *trip_distance_MAD*, correspondingly.*Modified Z-Scores* are computed for *trip distance* and *total amount* using the provided formula. Finally, data is filtered to retain only records where **the Z-Score for both trip distance and total amount is less than or equal to 3.5**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d56f062-6d2e-4290-9b1c-e5102c97593e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Your solution implementation to task 1.2 goes HERE\n",
    "def t12_remove_outliers(df):\n",
    "    # Calculation of median values for trip distance and total amount:\n",
    "    medians = df.select(pf.percentile_approx(\"trip_distance\", 0.5).alias('trip_distance_median'),pf.percentile_approx(\"total_amount\", 0.5).alias('total_amount_median')).collect()[0]\n",
    "\n",
    "    # Assigning median values to trip_distance_median, total_amount_median:\n",
    "    trip_distance_median = medians[0]\n",
    "    total_amount_median = medians[1]\n",
    "\n",
    "    # Difference of actual value and median of it:\n",
    "    df = df.withColumn('total_amount_median_diff', pf.round(pf.abs(pf.col('total_amount') - total_amount_median), 2))\n",
    "    df = df.withColumn('trip_distance_median_diff', pf.round(pf.abs(pf.col('trip_distance') - trip_distance_median), 2))\n",
    "\n",
    "    # Calculation of the median absolute deviation (MAD) for total amount and trip distance:\n",
    "    MAD = df.select(pf.percentile_approx(\"total_amount_median_diff\", 0.5).alias('total_amount_MAD'),pf.percentile_approx(\"trip_distance_median_diff\", 0.5).alias('trip_distance_MAD') ).collect()[0]\n",
    "\n",
    "    # Assigning MAD values to total_amount_MAD, trip_distance_MAD:\n",
    "    total_amount_MAD = MAD[0]\n",
    "    trip_distance_MAD = MAD[1]\n",
    "    \n",
    "    # Calculate Modified Z-score using the formula provided above (in task description)\n",
    "    df = df.withColumn('Z_score_trip_distance', pf.abs(pf.round((pf.col('trip_distance') - trip_distance_median) / (1.438 * trip_distance_MAD ),2 )))\n",
    "    df = df.withColumn('Z_score_total_amount', pf.abs(pf.round((pf.col('total_amount') - total_amount_median) / (1.438 * total_amount_MAD ),2 )))\n",
    "    \n",
    "    # Filtering only those data where Z score is less than or equal to 3.5:\n",
    "    df = df.filter(pf.col('Z_score_total_amount') <= 3.5)\n",
    "    df = df.filter(pf.col('Z_score_trip_distance') <= 3.5)\n",
    "    df = df.select('PULocationID', 'DOLocationID', 'trip_distance', 'passenger_count', 'total_amount')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96533a0e-242b-42d6-bc10-3b1ea3a13681",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 2,365,045\n"
     ]
    }
   ],
   "source": [
    "# execute task 1.2\n",
    "trips_12 = t12_remove_outliers(trips_11)\n",
    "\n",
    "print_count(trips_12)\n",
    "# display(trips_12.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77f3f561-5d39-46bf-84fa-0b0c3aafa0cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 2 - Compute new columns\n",
    "\n",
    "### Task 2.1 - Zone names\n",
    "\n",
    "Obtain the **start** and **end** zone names of each trip by joining the `trips` and `zone_names` datasets (i.e. by using the `zone_names` dataset as lookup table).\n",
    "\n",
    "**Note:** The columns containing the start and end zone ids of each trip are named `PULocationID` and `DOLocationID`, respectively.\n",
    "\n",
    "### Task 2.2 - Unit profitability\n",
    "\n",
    "Compute the column `unit_profitability = total_amount / trip_distance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a61d8288-1539-4f34-856f-98aa7fd0f7f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# develop your solution here (create/destroy cells as needed) and then implement it in the functions below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c094bb75-00ab-43cd-a9de-23a2261c060c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### Task 2.1\n",
    "\n",
    "The following function takes an argument dataframe (which is the output of task 1.2) and zone names dataset. Later, I've specified list of two columns *\"[\"PULocationID\", \"DOLocationID\"]\"* then I've looped through these columns and ***joined the trips data with zone names data***. Lastly, I've dropped certain columns which are not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf0cd0ff-8d0a-4b46-82de-e2edd56e2f10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Your solution implementation to task 2.1 goes HERE\n",
    "def t21_join_zones(df, zones_df = zone_names):\n",
    "    # input: output of task 1.2 and zone_names dataset\n",
    "    columns = [\"PULocationID\", \"DOLocationID\"]\n",
    "    # Joining trips with zones\n",
    "    for column in columns:\n",
    "        column_zone = column +'_zone'\n",
    "        df = (df.join(zone_names,(pf.col(column) == zones_df.LocationID),\"inner\").drop(\"LocationID\", \"Borough\", \"service_zone\").withColumnRenamed(\"zone\",column_zone))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a3abc15-94bb-4352-964f-e3919956b363",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 2,365,045\n"
     ]
    }
   ],
   "source": [
    "# execute task 2.1\n",
    "trips_21 = t21_join_zones(trips_12, zones_df = zone_names)\n",
    "\n",
    "print_count(trips_21)\n",
    "# display(trips_21.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ac58bfe-f704-4501-96f7-9e51f50a819a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### Task 2.2\n",
    "\n",
    "The below function **computes the unit profitability** using the *formula: total_amount / trip_distance*, and storing the result into the *unit_profitability* columns. The results are rounded to 2 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "244ecaed-2632-4ed1-ab91-e41d8a91b7c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Your solution implementation to task 2.2 goes HERE\n",
    "def t22_calc_profit(df):\n",
    "    # input: output of task 2.1\n",
    "    # Calculation of unit profitability:\n",
    "    df = df.withColumn(\"unit_profitability\", pf.round((df.total_amount / df.trip_distance), 2))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3528be7-4fe2-4e68-a94b-a3b33ada952c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 2,365,045\n"
     ]
    }
   ],
   "source": [
    "# execute task 2.2\n",
    "trips_22 = t22_calc_profit(trips_21)\n",
    "\n",
    "print_count(trips_22)\n",
    "# display(trips_22.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93c4a6e7-512f-40a8-ad2c-41a84ade373f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 3: Rank zones by traffic, passenger volume and profitability\n",
    "\n",
    "### 3.1 - Summarise interzonal travel\n",
    "\n",
    "Build a graph data structure of zone-to-zone traffic, representing aggregated data about trips between any two zones. The graph will have one node for each zone and one edge connecting each pair of zones. In addition, edges contain aggregate information about all trips between those zones. \n",
    "\n",
    "For example, zones Z1 and Z2 are connected by *two* edges: edge Z1 --> Z2 carries aggregate data about all trips that originated in Z1 and ended in Z2, and edge Z2 --> Z1 carries aggregate data about all trips that originated in Z2 and ended in Z1.\n",
    "\n",
    "The aggregate information of interzonal travel must include the following data:\n",
    "\n",
    "- `average_unit_profit` - the average unit profitability (calculated as `mean(unit_profitability)`).\n",
    "- `trips_count` -- the total number of recorded trips.\n",
    "- `total_passengers` -- the total number of passenger across all trips (sum of `passenger_count`).\n",
    "\n",
    "This graph can be represented as a new dataframe, with schema:\n",
    "\n",
    "\\[`PULocationID`, `DOLocationID`, `average_unit_profit`, `trips_count`, `total_passengers` \\]\n",
    "\n",
    "__hint__: the `groupby()` operator produces a `pyspark.sql.GroupedData` structure. You can then calculate multiple aggregations from this using `pyspark.sql.GroupedData.agg()`: \n",
    "- https://spark.apache.org/docs/3.2.0/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.groupby.html\n",
    "- https://spark.apache.org/docs/3.2.0/api/python/reference/api/pyspark.sql.GroupedData.agg.html\n",
    "\n",
    "### Task 3.2 - Obtain top-10 zones\n",
    "\n",
    "For each of the following measures, report the top-10 zones _using their plain names you dereferenced in the previous step, not the codes_. Note that this requires ranking the nodes in different orders. Specifically, you need to calculate the following further aggregations:\n",
    "\n",
    "- the **total** number of trips originating from Z. This is simply the sum of `trips_count` over all outgoing edges for Z, i.e., edges of the form Z -> \\*\n",
    "- the **average** profitability of a zone. This is the average of all `average_unit_profit` over all *outgoing* edges from Z.\n",
    "- The **total** passenger volume measured as the **sum** of `total_passengers` carried in trips that originate from Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "069fc68b-acdd-47c2-be12-27d32b7916c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# develop your solution here (create/destroy cells as needed) and then implement it in the functions below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa89e085-5914-4680-81dd-80f9d45f285f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### Task 3.1\n",
    "\n",
    "The function below accepts a DataFrame as an argument. It proceeds by grouping the DataFrame according to *PULocationID* and *DOLocationID*. In the initial DataFrame, named \"df1\", the **total number of trips** is tallied based on these locations. Subsequently, leveraging the `pf.mean` function, the **average unit profitability** and **total number of passengers** are computed for each location. Finally, the results from these two separate DataFrames are merged using the `join` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38084194-2635-46aa-ac74-93509c03a86e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Your solution to task 3.1 goes HERE\n",
    "def t31_summarise_trips(df):\n",
    "    # input: output of task 2.2\n",
    "    # Grouping dataframe based on PULocationID and DOLocationID, then performing required operations:\n",
    "    df1=df.groupby([\"PULocationID\", \"DOLocationID\"]).count().withColumnRenamed(\"count\", \"trips_count\")\n",
    "    df2=df.groupby([\"PULocationID\", \"DOLocationID\"]).agg(pf.mean(\"unit_profitability\").alias(\"average_unit_profit\"),\n",
    "                                                              pf.sum(\"passenger_count\").alias(\"total_passengers\"))\n",
    "    df=df1.join(df2,[\"PULocationID\", \"DOLocationID\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b81c74ad-e5f1-4b69-a320-bdf375b94e8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 9,857\n"
     ]
    }
   ],
   "source": [
    "# execute task 3.1\n",
    "graph = t31_summarise_trips(trips_22)\n",
    "\n",
    "print_count(graph)\n",
    "# display(graph.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4915f304-5801-4ff0-8643-f4461745958c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### Task 3.2\n",
    "\n",
    "The **main task of this section is to get top 10 ranked zones based on three different independent criterias**. The first function, named *t32_summarise_zones_pairs*, **associates zone IDs with their corresponding zone names** and returns the zone data. This zone data is then passed to the second function (*t32_top10_trips*) in order to achieve first criteria, responsible for **identifying the top 10 zones ranked by trip volume**. Subsequently, the same zone data are inputted into the third function (*t32_top10_profit*), which determines the **top 10 zones ranked by average unit profits**. Finally, the last function (*t32_top10_passenger*) calculates the **top 10 zones ranked by total passengers** for each zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c285cd2d-861b-4790-8fe8-4b4b6cdbc8f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Your solution to task 3.2 goes HERE (implement each of the functions below)\n",
    "def t32_summarise_zones_pairs(df, zones_df = zone_names):\n",
    "    columns = [\"PULocationID\", \"DOLocationID\"]\n",
    "    # Joining trips with zones\n",
    "    for column in columns:\n",
    "        column_zone = column +'_zone'\n",
    "        df = (df.join(zone_names, (pf.col(column) == zones_df.LocationID),\"inner\").drop(\"LocationID\", \"Borough\", \"service_zone\").withColumnRenamed(\"zone\",column_zone))\n",
    "    return df\n",
    "\n",
    "# Top 10 ranked zones by traffic (trip volume)\n",
    "def t32_top10_trips(df_zones):\n",
    "    # input: output of task 3.2\n",
    "    df_zones = zones.groupby([\"PULocationID_zone\"]).agg(pf.sum(\"trips_count\").alias(\"trips\")).sort([\"trips\"], ascending = False)\n",
    "    return df_zones.collect()[:10:]\n",
    "\n",
    "# Top 10 ranked zones by profit\n",
    "def t32_top10_profit(df_zones):\n",
    "    # input: output of task 3.2\n",
    "    df_zones = zones.groupby([\"PULocationID_zone\"]).agg(pf.avg(\"average_unit_profit\").alias(\"average\")).sort([\"average\"], ascending = False)\n",
    "    return df_zones.collect()[:10:]\n",
    "\n",
    "# Top 10 ranked zones by passenger volume\n",
    "def t32_top10_passenger(df_zones):\n",
    "    # input: output of task 3.2\n",
    "    df_zones = zones.groupby([\"PULocationID_zone\"]).agg(pf.sum(\"total_passengers\").alias(\"passengers\")).sort([\"passengers\"], ascending = False)\n",
    "    return df_zones.collect()[:10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bb32624-cca2-461e-816d-c49ecc98374e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# execute task 3.2\n",
    "zones = t32_summarise_zones_pairs(graph)\n",
    "\n",
    "top10_trips     = t32_top10_trips(zones)\n",
    "top10_profit    = t32_top10_profit(zones)\n",
    "top10_passenger = t32_top10_passenger(zones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "382adcca-e15c-45d2-88bf-6ae97b8c6e30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>PULocationID_zone</th><th>trips</th></tr></thead><tbody><tr><td>Upper East Side South</td><td>119710</td></tr><tr><td>Upper East Side North</td><td>101332</td></tr><tr><td>Penn Station/Madison Sq West</td><td>95537</td></tr><tr><td>Midtown Center</td><td>91937</td></tr><tr><td>Murray Hill</td><td>86045</td></tr><tr><td>Midtown East</td><td>83518</td></tr><tr><td>Lincoln Square East</td><td>79065</td></tr><tr><td>Lenox Hill West</td><td>72383</td></tr><tr><td>Clinton East</td><td>72335</td></tr><tr><td>Upper West Side South</td><td>72318</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Upper East Side South",
         119710
        ],
        [
         "Upper East Side North",
         101332
        ],
        [
         "Penn Station/Madison Sq West",
         95537
        ],
        [
         "Midtown Center",
         91937
        ],
        [
         "Murray Hill",
         86045
        ],
        [
         "Midtown East",
         83518
        ],
        [
         "Lincoln Square East",
         79065
        ],
        [
         "Lenox Hill West",
         72383
        ],
        [
         "Clinton East",
         72335
        ],
        [
         "Upper West Side South",
         72318
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "PULocationID_zone",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "trips",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use 'display()' or return a pandas DataFrame for 'pretty' output\n",
    "display(top10_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "847abfd1-7424-43c0-976e-5c615736a56b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>PULocationID_zone</th><th>average</th></tr></thead><tbody><tr><td>Queensboro Hill</td><td>259.1038888888889</td></tr><tr><td>Baisley Park</td><td>193.0674766995614</td></tr><tr><td>Prospect Park</td><td>169.1752314814815</td></tr><tr><td>Eastchester</td><td>107.23302083333334</td></tr><tr><td>Saint Michaels Cemetery/Woodside</td><td>73.3146103896104</td></tr><tr><td>Van Cortlandt Village</td><td>67.23423333333334</td></tr><tr><td>Crotona Park</td><td>62.13000000000002</td></tr><tr><td>Riverdale/North Riverdale/Fieldston</td><td>62.017999999999994</td></tr><tr><td>Flatbush/Ditmas Park</td><td>59.37259166666667</td></tr><tr><td>Bellerose</td><td>48.46958333333333</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Queensboro Hill",
         259.1038888888889
        ],
        [
         "Baisley Park",
         193.0674766995614
        ],
        [
         "Prospect Park",
         169.1752314814815
        ],
        [
         "Eastchester",
         107.23302083333334
        ],
        [
         "Saint Michaels Cemetery/Woodside",
         73.3146103896104
        ],
        [
         "Van Cortlandt Village",
         67.23423333333334
        ],
        [
         "Crotona Park",
         62.13000000000002
        ],
        [
         "Riverdale/North Riverdale/Fieldston",
         62.017999999999994
        ],
        [
         "Flatbush/Ditmas Park",
         59.37259166666667
        ],
        [
         "Bellerose",
         48.46958333333333
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "PULocationID_zone",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "average",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use 'display()' return a pandas DataFrame for 'pretty' output\n",
    "display(top10_profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90eb7561-5775-4e6a-8317-473f311e4cfe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>PULocationID_zone</th><th>passengers</th></tr></thead><tbody><tr><td>Upper East Side South</td><td>169790.0</td></tr><tr><td>Upper East Side North</td><td>143782.0</td></tr><tr><td>Penn Station/Madison Sq West</td><td>136813.0</td></tr><tr><td>Midtown Center</td><td>134603.0</td></tr><tr><td>Murray Hill</td><td>123489.0</td></tr><tr><td>Midtown East</td><td>119419.0</td></tr><tr><td>Lincoln Square East</td><td>114567.0</td></tr><tr><td>Clinton East</td><td>106834.0</td></tr><tr><td>Upper West Side South</td><td>105649.0</td></tr><tr><td>East Village</td><td>105261.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Upper East Side South",
         169790.0
        ],
        [
         "Upper East Side North",
         143782.0
        ],
        [
         "Penn Station/Madison Sq West",
         136813.0
        ],
        [
         "Midtown Center",
         134603.0
        ],
        [
         "Murray Hill",
         123489.0
        ],
        [
         "Midtown East",
         119419.0
        ],
        [
         "Lincoln Square East",
         114567.0
        ],
        [
         "Clinton East",
         106834.0
        ],
        [
         "Upper West Side South",
         105649.0
        ],
        [
         "East Village",
         105261.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "PULocationID_zone",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "passengers",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use 'display()' or return a pandas DataFrame for 'pretty' output\n",
    "display(top10_passenger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b950c5e-b7fd-405e-a0ba-0fe9c2752678",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 4 - Record the pipeline's execution time\n",
    "\n",
    "Record the execution time of:\n",
    "\n",
    "1. the whole pipeline\n",
    "2. the whole pipeline except task 1.2\n",
    "\n",
    "on the two tables below, for all dataset sizes: `'S'`, `'M'`, `'L'`, `'XL'`, `'XXL'`, and data formats: `parquet` and `delta`.\n",
    "\n",
    "Analyse the resulting execution times and comment on the effect of dataset size, dataset format and task complexity (with and without task 1.2) on pipeline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0e68e8c-4de9-4e04-9de0-f36e24b10f7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n    Trips dataset loaded!\n    ---\n      Size: XXL\n      Format: delta\n      Tables loaded: /FileStore/tables/taxi/delta/taxi-XXL-delta/\n      Number of trips (dataset rows): 132,396,785\n    \n"
     ]
    }
   ],
   "source": [
    "# CHANGE the value of the following arguments to record the pipeline execution times for increasing dataset sizes\n",
    "SIZE = 'XXL'\n",
    "DATA_FORMAT = 'delta'\n",
    "WITH_TASK_12 = True\n",
    "\n",
    "# Load trips dataset\n",
    "trips = init_trips(SIZE, DATA_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68681494-9d89-4706-908d-c5545444e8ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[Row(PULocationID_zone='Upper East Side South', trips=119710),\n",
       "  Row(PULocationID_zone='Upper East Side North', trips=101332),\n",
       "  Row(PULocationID_zone='Penn Station/Madison Sq West', trips=95537),\n",
       "  Row(PULocationID_zone='Midtown Center', trips=91937),\n",
       "  Row(PULocationID_zone='Murray Hill', trips=86045),\n",
       "  Row(PULocationID_zone='Midtown East', trips=83518),\n",
       "  Row(PULocationID_zone='Lincoln Square East', trips=79065),\n",
       "  Row(PULocationID_zone='Lenox Hill West', trips=72383),\n",
       "  Row(PULocationID_zone='Clinton East', trips=72335),\n",
       "  Row(PULocationID_zone='Upper West Side South', trips=72318)],\n",
       " [Row(PULocationID_zone='Queensboro Hill', average=259.1038888888889),\n",
       "  Row(PULocationID_zone='Baisley Park', average=193.0674766995614),\n",
       "  Row(PULocationID_zone='Prospect Park', average=169.1752314814815),\n",
       "  Row(PULocationID_zone='Eastchester', average=107.23302083333334),\n",
       "  Row(PULocationID_zone='Saint Michaels Cemetery/Woodside', average=73.3146103896104),\n",
       "  Row(PULocationID_zone='Van Cortlandt Village', average=67.23423333333334),\n",
       "  Row(PULocationID_zone='Crotona Park', average=62.13000000000002),\n",
       "  Row(PULocationID_zone='Riverdale/North Riverdale/Fieldston', average=62.017999999999994),\n",
       "  Row(PULocationID_zone='Flatbush/Ditmas Park', average=59.37259166666667),\n",
       "  Row(PULocationID_zone='Bellerose', average=48.46958333333333)],\n",
       " [Row(PULocationID_zone='Upper East Side South', passengers=169790.0),\n",
       "  Row(PULocationID_zone='Upper East Side North', passengers=143782.0),\n",
       "  Row(PULocationID_zone='Penn Station/Madison Sq West', passengers=136813.0),\n",
       "  Row(PULocationID_zone='Midtown Center', passengers=134603.0),\n",
       "  Row(PULocationID_zone='Murray Hill', passengers=123489.0),\n",
       "  Row(PULocationID_zone='Midtown East', passengers=119419.0),\n",
       "  Row(PULocationID_zone='Lincoln Square East', passengers=114567.0),\n",
       "  Row(PULocationID_zone='Clinton East', passengers=106834.0),\n",
       "  Row(PULocationID_zone='Upper West Side South', passengers=105649.0),\n",
       "  Row(PULocationID_zone='East Village', passengers=105261.0)]]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run and record the resulting execution time shown by databricks (on the cell footer)\n",
    "\n",
    "# IMPORTANT: this function calls all task functions in order of occurrence. For this code to run without errors, you have to load into memory all of the previous task-specific functions, even if you haven't implemented these yet.\n",
    "pipeline(trips, with_task_12 = WITH_TASK_12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08018100-ac87-4801-94fc-c9fec3dc6a66",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "_Table 1. Pipeline performance for `parquet` format._\n",
    "\n",
    "| metric                      | S    | M    | L    | XL   | XXL  |\n",
    "|-----------------------------|------|------|------|------|------|\n",
    "| rows (M)                    |  2.898 |  15.57 |  41.95 |  90.44 |  132.4 |\n",
    "| execution time   (w/o 1.2)  | 3.30 sec | 3.57 sec | 3.72 sec | 4.06 sec | 4.36 sec |\n",
    "| execution time              | 5.9 sec | 12.73 sec | 33.14 sec | 1.48 minute | 1.77 minute |\n",
    "| sec / 1M records (w/o 1.2)  | 1.14  | 0.23  | 0.09 | 0.05 | 0.04 |\n",
    "| sec / 1M records            | 2.04  | 0.82 | 0.79 | 0.98  | 0.80 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e565c658-ba51-45cc-a617-227d6befb6dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "_Table 2. Pipeline performance for `delta` format._\n",
    "\n",
    "| metric                      | S    | M    | L    | XL   | XXL  |\n",
    "|-----------------------------|------|------|------|------|------|\n",
    "| rows (M)                    |  2.898 |  15.57 |  41.95 |  90.44 |  132.4 |\n",
    "| execution time   (w/o 1.2)  | 3.52 sec | 3.60 sec | 3.72 sec | 3.76 sec | 4.28 sec |\n",
    "| execution time              | 5.28 sec | 6.61 sec | 8.19 sec | 8.25 sec | 8.99 sec |\n",
    "| sec / 1M records (w/o 1.2)  | 1.22  | 0.23 | 0.09 | 0.009 | 0.032 |\n",
    "| sec / 1M records            | 1.83 | 0.43 | 0.20 | 0.091 | 0.07 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc29c704-ac65-402e-8cdb-c59ef2b35ddc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "The above piepline was run on the cluster `CSC8641`\n",
    "\n",
    "Following things can be noted from the above performance table:\n",
    "\n",
    "1. The execution time for the pipeline (without task 1.2) is much less than that of the execution time of pipeline (with task 1.2)\n",
    "2. As we can see, as size of the dataset increases the execution time also increases which shows `linearity`.\n",
    "3. It can be seen from the above table that execution time required for `parquet` format is higher than `delta` file format.\n",
    "4. The execution time required for `parquet` format with task 1.2 for `XXL data size` is almost `1.77 minutes` whereas for `delta` format it's `below 9 seconds`.\n",
    "5. It means the `delta` file format is feasible to use for less execution time."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CSC8101-spark-coursework",
   "widgets": {}
  },
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
